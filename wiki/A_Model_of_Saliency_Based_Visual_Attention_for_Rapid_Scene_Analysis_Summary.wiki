#summary One-sentence summary of this page.
#sidebar TableOfContents

= Overview =

 <wiki:toc maxdepth="2"/>

= Details =

 * Abstract
 Multiscale image features are combined into a single topographical saliency map
 A dynamical neural network selects attended locations in order of decreasing saliency. 

 == Introduction ==
 Primates gain the remarkable ability to interpret complex scene in real-time system during million years of evolution. Despite the limit of human brains, it seem to be done by using the "focus of attention", circumscribed region. After the region is discovered  by a rapid, bottom-up, saliency driven, and task independent manner, a slower top-down, volition-controller, and task dependent manner such as lane detection and tracking will be used to find specific objects.

 The "feature itegration theory" built on a second biologically plausible architecture proposed by Koch and Ullman, explains human visual search strategies. At beginning, different spatial locations compete for the saliency and builds their own feature maps ( saliency maps for each feature ). Then all feature maps are integrated into a single "saliency map" which represents local conspicuity over the entire image. 

 This model shows us the complete bottom-up saliency visual attention method without any prerequisite knowledge. However, it is flexible enough to be combined with any top-down guidance to shift attention. The prior knowledge is usually used to weight the importance of different features; therefore, those with high-weights could persist. 

 == Models ==

 Input, 640x480 static color images, are processed by *dyadic Guassian pyramids*, to create 9 spatial scales ranging from 1:1 (scale sero) to 1:256 (scale eight).
 Each features such as colors, intensity, and orientations is computed by a set of linear *"center-surround" operations* akin to visual receptive fields. 
 
 Center-surround is implemented in the model as difference between fine and coarse scales: 
  * The center c is a pixel at scale {2,3,4}.
  * The surround s is a pixel at scale {3,4}.
  * The across-scale difference between two maps is obtained by *interpolation to the finer scale and point-by-point substraction*.

 Extraction of Early Visual Features


Add your content here.  Format your content with:
  * Text in *bold* or _italic_
  * Headings, paragraphs, and lists
  * Automatic links to other wiki pages
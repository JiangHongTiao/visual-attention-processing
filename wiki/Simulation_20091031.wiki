#summary Applying Itti iLab toolbox for movies from Automotive Database of The University of Nottingham, Malaysia Campus
#labels Simulations

= Introduction =

<wiki:toc depth='1'/>

= Details =

== Plan == 

Running the simulations and collect how many times the machine can get the correct positions of objects ( lane marks and vehicles )

== Input Data ==

There are three input video: blurredVideo.avi, clearVideo.avi, shadowyVideo.avi 

== Simulation ==

ezvision -K --in=path/[video].avi --out=raster:path/ --output-frames=0-30@EVENT

-K shows all necessary steps before the attention point is extracted
--output-frames=0-30@EVENT: give 31 output frames and the frame is generated when the attention point is shifted.

== Output Data == 

I have created to store output frames of three input videos and following tables show the simulation results

|| *Video Name* || *No times of vehicle attention* || *No times of lane marks attention* || Blurred_Videos || 8/30 || 6/30 ||
|| Clear_Videos || 8/30 || 14/30 ||
|| Shadowy_Videos || 5/30 || 11/30 ||

== Conclusion ==

After three testing cases, we can generally conclude that visual saliency alone can not act as vehicle / lane detectors well.